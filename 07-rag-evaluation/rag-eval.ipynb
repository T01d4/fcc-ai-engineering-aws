{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the models aavailable in Amazon Bedrock\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "# Create a Bedrock client\n",
    "bedrock_client = boto3.client('bedrock')\n",
    "\n",
    "# Create a Bedrock Runtime client\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all the models in a variable\n",
    "all_models = bedrock_client.list_foundation_models()\n",
    "all_models_summaries = all_models['modelSummaries'] \n",
    "\n",
    "# Convert the model summaries to a DataFrame with only the columns we want\n",
    "df = pd.DataFrame(all_models_summaries, columns=['modelName', 'providerName', 'modelId', 'modelArn', 'inputModalities', 'outputModalities'])\n",
    "\n",
    "# Show the DataFrame without truncating the output and display all rows\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Print the models that are provided by Amazon and show the first 10 rows and select the columns modelName, providerName, modelId\n",
    "df[df['providerName'] == 'Amazon'][:10][['modelName', 'providerName', 'modelId']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the embedding models\n",
    "df[df['modelId'].str.contains('emb')][['modelName', 'providerName', 'modelId', 'inputModalities', 'outputModalities']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "\n",
    "def create_vector_store(index_name=\"2023WC\"):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    \n",
    "    # Load the HTML file\n",
    "    loader = BSHTMLLoader(\"data/2023WC.html\")\n",
    "    data = loader.load()\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    \n",
    "    # Initialize embeddings\n",
    "    embeddings = BedrockEmbeddings(model_id='amazon.titan-embed-text-v2:0')\n",
    "    \n",
    "    # Create FAISS vector store\n",
    "    db = FAISS.from_documents(chunks, embeddings)\n",
    "    \n",
    "    # Save the vector store\n",
    "    db.save_local(\"data\", index_name)\n",
    "    print(\"Vector store created successfully!\")\n",
    "    \n",
    "    return db\n",
    "    \n",
    "\n",
    "index = create_vector_store(index_name=\"2023WC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain_aws import BedrockEmbeddings, ChatBedrock\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and standardize text by removing unwanted elements\n",
    "def clean_text(text):\n",
    "    # Convert non-breaking spaces to regular spaces\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove citation references like [1], [2,3], etc.\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    \n",
    "    # Normalize whitespace by removing extra spaces and line breaks\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "def rag_function(query, index):\n",
    "    \n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = index.similarity_search(query, k=2)\n",
    "    retrieved_context = [clean_text(retrieved_docs[0].page_content + retrieved_docs[1].page_content)]\n",
    "\n",
    "    # Create augmented prompt\n",
    "    augmented_prompt = f\"\"\"\n",
    "    Given the context below answer the question.\n",
    "\n",
    "    Question: {query} \n",
    "\n",
    "    Context: {retrieved_context}\n",
    "\n",
    "    Remember to answer only based on the context provided and not from any other source. \n",
    "\n",
    "    If the question cannot be answered based on the provided context, say I don't know.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize Bedrock Claude model\n",
    "    llm = ChatBedrock(\n",
    "        model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        model_kwargs={\n",
    "            \"temperature\": 0,\n",
    "            \"max_tokens\": 2048\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Generate response\n",
    "    response = llm.invoke(augmented_prompt)\n",
    "\n",
    "    return retrieved_context, response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_rag_results(context, answer):\n",
    "    # Display the context and answer with markdown\n",
    "    from IPython.display import Markdown\n",
    "\n",
    "    md_text = f\"\"\"\n",
    "    ### Context:\n",
    "    {context}\n",
    "\n",
    "    ### Answer:\n",
    "    {answer}\n",
    "    \"\"\"\n",
    "\n",
    "    display(Markdown(md_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What RAG?\"\n",
    "context, answer = rag_function(query, index)\n",
    "\n",
    "\n",
    "display_rag_results(context, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who won the world cup?\"\n",
    "context, answer = rag_function(query, index)\n",
    "\n",
    "\n",
    "display_rag_results(context, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What was Virat Kohli's achievement in the Cup?\"\n",
    "context, answer = rag_function(query, index)\n",
    "\n",
    "\n",
    "display_rag_results(context, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
